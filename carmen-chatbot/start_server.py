import os
import sys
import json
import requests
import uvicorn
from pathlib import Path
from dotenv import set_key, load_dotenv

from InquirerPy import inquirer
from InquirerPy.base.control import Choice

# Paths
BASE_DIR = Path(__file__).resolve().parent
ENV_PATH = BASE_DIR.parent / ".env"

# Load current env context
load_dotenv(ENV_PATH)

def fetch_openrouter_models():
    print("Fetching OpenRouter models...")
    try:
        base_url = os.environ.get("OPENROUTER_BASE_URL", "https://openrouter.ai")
        resp = requests.get(f"{base_url}/api/v1/models", timeout=10)
        resp.raise_for_status()
        data = resp.json()
        models = data.get("data", [])
        return [Choice(value=m["id"], name=f"{m['id']} ({m.get('name', 'N/A')})") for m in models]
    except Exception as e:
        print(f"Error fetching OpenRouter models: {e}")
        return []

def fetch_ollama_models():
    print("Fetching local Ollama models...")
    ollama_url = os.getenv("OLLAMA_URL", "http://localhost:11434")
    try:
        resp = requests.get(f"{ollama_url}/api/tags", timeout=5)
        resp.raise_for_status()
        data = resp.json()
        models = data.get("models", [])
        return [Choice(value=m["name"], name=m["name"]) for m in models]
    except Exception as e:
        print(f"Error fetching Ollama models at {ollama_url}: {e}")
        return []

def check_llm_health(provider: str, model: str) -> bool:
    print(f"\nü©∫ Performing health check for {provider.upper()} model '{model}'...")
    try:
        if provider == "openrouter":
            api_key = os.environ.get("OPENROUTER_API_KEY")
            if not api_key:
                print("‚ùå ERROR: OPENROUTER_API_KEY is not set in .env")
                return False
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "HTTP-Referer": "http://localhost:8000",
                "X-Title": "Carmen Chatbot"
            }
            data = {
                "model": model,
                "messages": [{"role": "user", "content": "Hello"}],
                "max_tokens": 5
            }
            base_url = os.environ.get("OPENROUTER_BASE_URL", "https://openrouter.ai")
            resp = requests.post(f"{base_url}/api/v1/chat/completions", headers=headers, json=data, timeout=10)
            if resp.status_code == 200:
                print("‚úÖ Health check passed! Model responded successfully.")
                return True
            else:
                print(f"‚ùå Health check failed with HTTP {resp.status_code}: {resp.text}")
                return False
                
        elif provider == "ollama":
            ollama_url = os.environ.get("OLLAMA_URL", "http://localhost:11434")
            data = {
                "model": model,
                "prompt": "Hello",
                "stream": False,
                "options": {"num_predict": 5}
            }
            resp = requests.post(f"{ollama_url}/api/generate", json=data, timeout=15)
            if resp.status_code == 200:
                print("‚úÖ Health check passed! Model responded successfully.")
                return True
            else:
                print(f"‚ùå Health check failed with HTTP {resp.status_code}: {resp.text}")
                return False
                
    except Exception as e:
        print(f"‚ùå Health check exception: {e}")
        return False
        
    return False

def main():
    # üåü Check Environment Mode
    env_mode = os.environ.get("ENVIRONMENT", "development").lower()
    
    if env_mode == "production":
        print("\nüöÄ [PRODUCTION MODE] Skipping Interactive Prompts üöÄ\n")
        provider = os.environ.get("ACTIVE_LLM_PROVIDER", "openrouter")
        
        if provider == "openrouter":
            model = os.environ.get("OPENROUTER_CHAT_MODEL", "stepfun/step-3.5-flash:free")
        else:
            provider = "ollama" # fallback
            model = os.environ.get("OLLAMA_CHAT_MODEL", "gemma3:1b")
            
        print(f"üìå Using Provider: {provider.upper()}")
        print(f"üìå Using Model: {model}\n")
        
        # Pre-Flight Health Check (Non-blocking in Production)
        print("‚è≥ Running pre-flight health check...")
        is_healthy = check_llm_health(provider, model)
        if not is_healthy:
            print("‚ö†Ô∏è WARNING: Health check failed, but forcing start in PRODUCTION mode...")
            
    else:
        # üíª Development Mode (Interactive)
        print("\nü§ñ Welcome to Carmen LLM Configurator (Development Mode) ü§ñ\n")
        
        while True:
            provider = inquirer.select(
                message="Which LLM Provider would you like to use today?",
                choices=[
                    Choice(value="openrouter", name="OpenRouter (Cloud API)"),
                    Choice(value="ollama", name="Ollama (Local Server)")
                ],
                default="openrouter"
            ).execute()
        
            if provider == "openrouter":
                choices = fetch_openrouter_models()
                if not choices:
                    print("Failed to load models. Exiting.")
                    sys.exit(1)
                    
                model = inquirer.fuzzy(
                    message="Search and select an OpenRouter model:",
                    choices=choices,
                    match_exact=True,
                ).execute()
                
            elif provider == "ollama":
                choices = fetch_ollama_models()
                if not choices:
                    print("No models found. Please ensure Ollama is running and models are pulled.")
                    sys.exit(1)
                    
                model = inquirer.fuzzy(
                    message="Search and select a local Ollama model:",
                    choices=choices,
                    match_exact=True,
                ).execute()
                
            save_env = inquirer.confirm(
                message="Do you want to save this configuration to .env as the new default?",
                default=True
            ).execute()
            
            # Apply configurations ONLY in memory for this run session
            os.environ["ACTIVE_LLM_PROVIDER"] = provider
            if save_env:
                set_key(ENV_PATH, "ACTIVE_LLM_PROVIDER", provider)
            
            if provider == "openrouter":
                os.environ["OPENROUTER_CHAT_MODEL"] = model
                if save_env:
                    set_key(ENV_PATH, "OPENROUTER_CHAT_MODEL", model)
            else:
                os.environ["OLLAMA_CHAT_MODEL"] = model
                if save_env:
                    set_key(ENV_PATH, "OLLAMA_CHAT_MODEL", model)
                
            # Pre-Flight Health Check
            is_healthy = check_llm_health(provider, model)
            if not is_healthy:
                action = inquirer.select(
                    message="‚ö†Ô∏è The selected model failed the health check or is unreachable. What would you like to do?",
                    choices=[
                        Choice(value="retry", name="Select a different model"),
                        Choice(value="force", name="Force start the server anyway"),
                        Choice(value="abort", name="Abort and exit")
                    ],
                    default="retry"
                ).execute()
                
                if action == "retry":
                    print("\nRestarting selection process...\n")
                    continue
                elif action == "abort":
                    print("\nüõë Server startup aborted by user.")
                    sys.exit(1)
                # if "force", it just breaks out of the loop and starts
                
            break # Exit loop if healthy or if user chose "force"
        
    print(f"\nüöÄ Starting Backend Server with [{provider.upper()}] -> [{model}]...\n")
    
    # Start Uvicorn
    # Use reload=True only in development mode
    should_reload = (env_mode != "production")
    if should_reload:
        print("üîÑ Hot-reload is ENABLED (Development Mode)")
        uvicorn.run("backend.main:app", host="0.0.0.0", port=8000, reload=True)
    else:
        print("‚è© Hot-reload is DISABLED, starting with multiple workers (Production Mode)")
        # In production on Windows, Gunicorn isn't available, so we use uvicorn with workers
        uvicorn.run("backend.main:app", host="0.0.0.0", port=8000, reload=False, workers=4)

if __name__ == "__main__":
    main()
